{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install bert\n",
        "!pip install opendatasets\n",
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9PBcAuPS5mF",
        "outputId": "e15c7256-7cc8-49e8-d00d-e3319a65c51f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bert\n",
            "  Downloading bert-2.2.0.tar.gz (3.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting erlastic\n",
            "  Downloading erlastic-2.0.0.tar.gz (6.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: bert, erlastic\n",
            "  Building wheel for bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert: filename=bert-2.2.0-py3-none-any.whl size=3764 sha256=116911fcc68d67b15be386b1bb9fe9e52cd97b710e14eccc773ac75556afff06\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/11/40/6439aef2635f7f0137a79c4defb4c4e65dd051ec0198429e3b\n",
            "  Building wheel for erlastic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for erlastic: filename=erlastic-2.0.0-py3-none-any.whl size=6795 sha256=c5e22e6c36d9b51c35197817bc1f3cb0af242f4770db6bf1a14d6911213e88e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/ee/c9/a6/41a81618e939b746a3151700565d191bca832b6c345ea9b87a\n",
            "Successfully built bert erlastic\n",
            "Installing collected packages: erlastic, bert\n",
            "Successfully installed bert-2.2.0 erlastic-2.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from opendatasets) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from opendatasets) (4.64.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.8/dist-packages (from opendatasets) (1.5.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (2022.12.7)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (8.0.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (2.25.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (1.24.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle->opendatasets) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle->opendatasets) (4.0.0)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "import pandas\n",
        "  \n",
        "od.download(\"https://www.kaggle.com/datasets/joydeb28/nlp-benchmarking-data-for-intent-and-entity\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-d5tlmO11qc",
        "outputId": "aa163f5a-b6ee-4f51-c556-2427f8fcf12a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: kilwazoldik\n",
            "Your Kaggle Key: ··········\n",
            "Downloading nlp-benchmarking-data-for-intent-and-entity.zip to content/data/nlp-benchmarking-data-for-intent-and-entity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 356k/356k [00:00<00:00, 35.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import json\n",
        "import os\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import bert\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "print(\"TensorFlow Version:\",tf.__version__)\n",
        "print(\"Hub version: \",hub.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8KQJadLR9w8",
        "outputId": "4444d25b-271c-4036-80c0-5bf0e47cd2dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.11.0\n",
            "Hub version:  0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LoadingData():\n",
        "            \n",
        "    def __init__(self):\n",
        "        train_file_path = os.path.join(\"..\",\"/content\",\"nlp-benchmarking-data-for-intent-and-entity\",\"benchmarking_data\",\"Train\")\n",
        "        validation_file_path = os.path.join(\"..\",\"/content\",\"nlp-benchmarking-data-for-intent-and-entity\",\"benchmarking_data\",\"Validate\")\n",
        "        category_id = 0\n",
        "        self.cat_to_intent = {}\n",
        "        self.intent_to_cat = {}\n",
        "        \n",
        "        for dirname, _, filenames in os.walk(train_file_path):\n",
        "            for filename in filenames:\n",
        "                file_path = os.path.join(dirname, filename)\n",
        "                intent_id = filename.replace(\".json\",\"\")\n",
        "                self.cat_to_intent[category_id] = intent_id\n",
        "                self.intent_to_cat[intent_id] = category_id\n",
        "                category_id+=1\n",
        "        print(self.cat_to_intent)\n",
        "        print(self.intent_to_cat)\n",
        "        '''Training data'''\n",
        "        training_data = list() \n",
        "        for dirname, _, filenames in os.walk(train_file_path):\n",
        "            for filename in filenames:\n",
        "                file_path = os.path.join(dirname, filename)\n",
        "                intent_id = filename.replace(\".json\",\"\")\n",
        "                training_data+=self.make_data_for_intent_from_json(file_path,intent_id,self.intent_to_cat[intent_id])\n",
        "        self.train_data_frame = pd.DataFrame(training_data, columns =['query', 'intent','category'])   \n",
        "        \n",
        "        self.train_data_frame = self.train_data_frame.sample(frac = 1)\n",
        "        '''Validation data'''\n",
        "        validation_data = list()    \n",
        "        for dirname, _, filenames in os.walk(validation_file_path):\n",
        "            for filename in filenames:\n",
        "                file_path = os.path.join(dirname, filename)\n",
        "                intent_id = filename.replace(\".json\",\"\")\n",
        "                validation_data +=self.make_data_for_intent_from_json(file_path,intent_id,self.intent_to_cat[intent_id])                \n",
        "        self.validation_data_frame = pd.DataFrame(validation_data, columns =['query', 'intent','category'])\n",
        "\n",
        "        self.validation_data_frame = self.validation_data_frame.sample(frac = 1)\n",
        "        \n",
        "        \n",
        "    def make_data_for_intent_from_json(self,json_file,intent_id,cat):\n",
        "        json_d = json.load(open(json_file))         \n",
        "        \n",
        "        json_dict = json_d[intent_id]\n",
        "\n",
        "        sent_list = list()\n",
        "        for i in json_dict:\n",
        "            each_list = i['data']\n",
        "            sent =\"\"\n",
        "            for i in each_list:\n",
        "              sent = sent + i['text']+ \" \"\n",
        "            sent =sent[:-1]\n",
        "            for i in range(3):\n",
        "                sent = sent.replace(\"  \",\" \")\n",
        "            sent_list.append((sent,intent_id,cat))\n",
        "        return sent_list"
      ],
      "metadata": {
        "id": "btWXik6qSLcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_data_obj = LoadingData()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9MJQuDzS9P8",
        "outputId": "831864dc-f3ce-43d0-a742-e3ae6de36bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'BookRestaurant', 1: 'SearchScreeningEvent', 2: 'PlayMusic', 3: 'GetWeather', 4: 'SearchCreativeWork', 5: 'AddToPlaylist', 6: 'RateBook'}\n",
            "{'BookRestaurant': 0, 'SearchScreeningEvent': 1, 'PlayMusic': 2, 'GetWeather': 3, 'SearchCreativeWork': 4, 'AddToPlaylist': 5, 'RateBook': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_data_obj.train_data_frame.to_csv('/content/drive/MyDrive/Datasets/NLP Auto Complete/train.csv')\n",
        "load_data_obj.validation_data_frame.to_csv('/content/drive/MyDrive/Datasets/NLP Auto Complete/validation.csv')"
      ],
      "metadata": {
        "id": "WIi9da7vS-Eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sdVgvHqMTUWg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}